{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c92fb7-7899-4f9f-ad23-cd30337e3152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "filtered_df = pd.read_csv('filtered_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29012321-dea4-436d-a197-ab3d321b8fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Year'].between(2018, 2022)]\n",
    "df = df[df['Year'] != 2020]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d9c531-ad34-4ee1-8f12-bc1bfb2a06c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Hx_GDM' and 'Other Endocrine probs' to object type\n",
    "df['Hx_GDM'] = df['Hx_GDM'].astype('object')\n",
    "df['Other Endocrine probs'] = df['Other Endocrine probs'].astype('object')\n",
    "\n",
    "# Verify the changes\n",
    "print(df.dtypes[['Hx_GDM', 'Other Endocrine probs']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b52bc1-c4a4-4a5d-9313-b0f15289c8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes on relevant key (assuming there's an 'ID' column or similar to merge on)\n",
    "merged_df = df.merge(filtered_df[['ID', 'Date of Birth','GDM Val', 'matched']], on=['ID', 'Date of Birth'], how='left')\n",
    "\n",
    "# Update GDM labels\n",
    "merged_df.loc[(merged_df['GDM Val'] == 1) & (merged_df['GDM'] == 0), 'GDM'] = 1\n",
    "merged_df = merged_df[~((merged_df['GDM'] == 1) & (merged_df['GDM Val'] == 0))]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "merged_df.drop(columns=['GDM Val', 'matched'], inplace=True)\n",
    "merged_df = merged_df.drop(['Date of Birth', 'Endocrine probs.', 'Year', 'Height of Mother', 'Ever Smoked_x', 'Epid/Spinal Complications',\n",
    "              'Onset of Labour', 'Multiple Pregnancy','Blood Loss', 'Maternal blood taken','Maternal investigations','Post Delivery Problems',\n",
    "               'Perineum/other tears', 'Occupation - Mother', 'CTG/ECG anomalies', 'Shoulder Difficulty', 'Weight of Baby', 'Sex of Baby',\n",
    "               'Apgar at 1 minute', 'Apgar at 5 minutes', 'Outcome', 'Resuscitation', 'Transferred to', 'ISCO Title', 'Medical probs in Preg', 'Drugs this pregnancy', 'ISCO Group'], axis=1)\n",
    "\n",
    "merged_df = merged_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7364f066-9974-43e2-b010-1da04dca488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset the data for training\n",
    "df = df.drop(['Date of Birth', 'Year', 'Endocrine probs.', 'Height of Mother', 'Ever Smoked_x', 'Epid/Spinal Complications',\n",
    "              'Onset of Labour', 'Multiple Pregnancy','Blood Loss', 'Maternal blood taken','Maternal investigations','Post Delivery Problems',\n",
    "               'Perineum/other tears', 'Occupation - Mother', 'CTG/ECG anomalies', 'Shoulder Difficulty', 'Weight of Baby', 'Sex of Baby',\n",
    "               'Apgar at 1 minute', 'Apgar at 5 minutes', 'Outcome', 'Resuscitation', 'Transferred to', 'ISCO Title', 'Medical probs in Preg', 'Drugs this pregnancy', 'ISCO Group'], axis=1)\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ce339-8e97-4617-946b-82c40456ff5c",
   "metadata": {},
   "source": [
    "# Train on Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365a81a-4a5c-40b6-b7e6-2ddd9afff695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LogisticRegression as SKLogit\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.special              import logit, expit\n",
    "\n",
    "\n",
    "# Combine the datasets and create a source column\n",
    "df['source'] = 'raw'\n",
    "merged_df['source'] = 'validated'\n",
    "combined_df = pd.concat([df, merged_df], ignore_index=True)\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = combined_df.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = combined_df.select_dtypes(exclude=['object']).columns.tolist()\n",
    "categorical_features.remove('source')\n",
    "numerical_features.remove('GDM')\n",
    "numerical_features.remove('ID')\n",
    "\n",
    "# Preprocessor setup\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features),\n",
    "        ('num', StandardScaler(), numerical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Apply preprocessing to the entire dataset except 'GDM' and 'ID'\n",
    "X = combined_df.drop(columns=['GDM', 'ID'])\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "y = combined_df['GDM'].values\n",
    "\n",
    "# Convert processed features back to DataFrame with transformed column names\n",
    "columns_transformed = (list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))\n",
    "                       + numerical_features + ['source'])\n",
    "X_processed_df = pd.DataFrame(X_processed, columns=columns_transformed)\n",
    "X_processed_df['ID'] = combined_df['ID'].values\n",
    "X_processed_df['source'] = combined_df['source'].values\n",
    "\n",
    "# Perform group-based split to create train and test sets\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.7, random_state=42)\n",
    "train_idxs, test_idxs = next(gss.split(X_processed_df, groups=X_processed_df['ID']))\n",
    "\n",
    "train_set = X_processed_df.iloc[train_idxs]\n",
    "test_set = X_processed_df.iloc[test_idxs]\n",
    "\n",
    "# Now subset the train and test sets based on the 'source' column\n",
    "# Train on raw data\n",
    "raw_mask = train_set['source'] == 'raw'\n",
    "X_train_raw = train_set[raw_mask].drop(columns=['ID', 'source'])\n",
    "y_train_raw = y[train_set[raw_mask].index]\n",
    "\n",
    "# Test on validated data\n",
    "validated_mask = test_set['source'] == 'validated'\n",
    "X_test_validated = test_set[validated_mask].drop(columns=['ID', 'source'])\n",
    "y_test_validated = y[test_set[validated_mask].index]\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logistic_regression = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation on raw training data\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "roc_auc_scores = cross_val_score(logistic_regression, X_train_raw, y_train_raw, cv=kfold, scoring='roc_auc')\n",
    "avg_precision_scores = cross_val_score(logistic_regression, X_train_raw, y_train_raw, cv=kfold, scoring='average_precision')\n",
    "\n",
    "# Train the model on the entire raw training set\n",
    "logistic_regression.fit(X_train_raw, y_train_raw)\n",
    "\n",
    "# Predict on the validated test set\n",
    "y_pred_prob_validated = logistic_regression.predict_proba(X_test_validated)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_validated = roc_auc_score(y_test_validated, y_pred_prob_validated)\n",
    "avg_precision_validated = average_precision_score(y_test_validated, y_pred_prob_validated)\n",
    "\n",
    "print(f\"ROC AUC (Validated Data): {roc_auc_validated:.3f}\")\n",
    "print(f\"Average Precision (Validated Data): {avg_precision_validated:.3f}\")\n",
    "print(f\"Cross-validated ROC AUC scores (Raw Data): {roc_auc_scores}\")\n",
    "print(f\"Cross-validated Average Precision scores (Raw Data): {avg_precision_scores}\")\n",
    "\n",
    "# ─── Calibration curve (non-parametric bins) ────────────────────────────────\n",
    "prob_true, prob_pred = calibration_curve(\n",
    "    y_test_validated,\n",
    "    y_pred_prob_validated,\n",
    "    n_bins=10,\n",
    "    strategy='uniform'\n",
    ")\n",
    "\n",
    "# linear least-squares in prob space\n",
    "lr = LinearRegression().fit(prob_pred.reshape(-1,1), prob_true)\n",
    "b0, b1 = lr.intercept_, lr.coef_[0]\n",
    "print(f\"Linear calib intercept: {b0:.3f}\")\n",
    "print(f\"Linear calib slope:     {b1:.3f}\")\n",
    "\n",
    "# 5b) fit logistic in log-odds space\n",
    "calib = LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000)\n",
    "eps   = 1e-15\n",
    "p_clip = np.clip(y_pred_prob_validated, eps, 1-eps)\n",
    "logodds = logit(p_clip).reshape(-1,1)\n",
    "calib.fit(logodds, y_test_validated)\n",
    "\n",
    "α = calib.intercept_[0]\n",
    "β = calib.coef_[0,0]\n",
    "print(f\"Calibration intercept (logit scale): {α:.3f}\")\n",
    "print(f\"Calibration slope     (logit scale): {β:.3f}\")\n",
    "\n",
    "# O : E ratio\n",
    "exp_pos = np.sum(y_pred_prob_validated)\n",
    "obs_pos = np.sum(y_test_validated)\n",
    "oe_ratio = obs_pos/exp_pos if exp_pos>0 else np.nan\n",
    "print(f\"O:E ratio (obs/exp): {oe_ratio:.3f}\")\n",
    "\n",
    "\n",
    "# ─── Plot all three panels: ROC, PR, and new Calibration ────────────────────\n",
    "fpr, tpr, _        = roc_curve(y_test_validated, y_pred_prob_validated)\n",
    "precision, recall, _ = precision_recall_curve(y_test_validated, y_pred_prob_validated)\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# ROC\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'ROC AUC = {roc_auc_validated:.3f}')\n",
    "plt.plot([0,1],[0,1],'k--', lw=0.8)\n",
    "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve'); plt.legend(loc='lower right')\n",
    "\n",
    "# Precision-Recall\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(recall_rf, precision_rf, label=f'AP = {avg_precision_validated:.3f}')\n",
    "plt.xlabel('Recall'); plt.ylabel('Precision')\n",
    "plt.title('Precision–Recall Curve'); plt.legend(loc='lower left')\n",
    "\n",
    "# Calibration\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(prob_pred, prob_true, marker='o', linestyle='-',\n",
    "         label='Calibration')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=0.8, label='Ideal')\n",
    "\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('Observed proportion')\n",
    "plt.title('Calibration curve')\n",
    "plt.text(0.05, 0.85,\n",
    "         f'Intercept = {α:.2f}\\nSlope = {β:.2f}',\n",
    "         transform=plt.gca().transAxes,\n",
    "         bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26965a76-e59f-4249-859a-a5c527ec6776",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gdm_true = y_test_validated\n",
    "ehr_gdm_prob = y_pred_prob_validated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e839539-0dc0-4155-8da5-7eb5891bbeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(13, 7), dpi=300)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, label=f'ROC AUC = {roc_auc_validated:.3f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=24)\n",
    "plt.ylabel('True Positive Rate', fontsize=24)\n",
    "plt.title('ROC Curve', fontsize=28)\n",
    "plt.legend(loc=\"lower right\", fontsize=20)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(recall, precision, label=f'Average Precision = {avg_precision_validated:.3f}')\n",
    "plt.xlabel('Recall', fontsize=24)\n",
    "plt.ylabel('Precision', fontsize=24)\n",
    "plt.title('Precision-Recall Curve', fontsize=28)\n",
    "plt.legend(loc=\"lower left\", fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98678af6-5480-443c-b1f8-39b25b000ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 7), dpi=300)\n",
    "plt.plot(prob_pred, prob_true, marker='o', linestyle='-', label='Calibration')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=0.8, label='Ideal')\n",
    "plt.xlabel('Predicted probability'); plt.ylabel('Observed proportion')\n",
    "plt.title('Calibration curve')\n",
    "plt.text(0.05, 0.85,\n",
    "         f'Intercept = {α:.2f}\\nSlope = {β:.2f}',\n",
    "         transform=plt.gca().transAxes,\n",
    "         bbox=dict(boxstyle='round', alpha=0.1))\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c406a5-3954-49cd-aeee-daad64fe62d4",
   "metadata": {},
   "source": [
    "## RF Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7da60d-4b8c-4bd4-b5fe-1c158756ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "roc_auc_scores_rf = cross_val_score(random_forest, X_train_raw, y_train_raw, cv=kfold, scoring='roc_auc')\n",
    "avg_precision_scores_rf = cross_val_score(random_forest, X_train_raw, y_train_raw, cv=kfold, scoring='average_precision')\n",
    "\n",
    "# Train the model on the entire training set\n",
    "random_forest.fit(X_train_raw, y_train_raw)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob_rf = random_forest.predict_proba(X_test_validated)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_rf = roc_auc_score(y_test_validated, y_pred_prob_rf)\n",
    "avg_precision_rf = average_precision_score(y_test_validated, y_pred_prob_rf)\n",
    "\n",
    "print(f\"ROC AUC (Random Forest): {roc_auc_rf:.3f}\")\n",
    "print(f\"Average Precision (Random Forest): {avg_precision_rf:.3f}\")\n",
    "print(f\"Cross-validated ROC AUC scores (Random Forest): {roc_auc_scores_rf}\")\n",
    "print(f\"Cross-validated Average Precision scores (Random Forest): {avg_precision_scores_rf}\")\n",
    "\n",
    "\n",
    "# ─── Calibration curve (non-parametric bins) ────────────────────────────────\n",
    "prob_true, prob_pred = calibration_curve(\n",
    "    y_test_validated,\n",
    "    y_pred_prob_rf,\n",
    "    n_bins=10,\n",
    "    strategy='uniform'\n",
    ")\n",
    "\n",
    "# linear least-squares in prob space\n",
    "lr = LinearRegression().fit(prob_pred.reshape(-1,1), prob_true)\n",
    "b0, b1 = lr.intercept_, lr.coef_[0]\n",
    "print(f\"Linear calib intercept: {b0:.3f}\")\n",
    "print(f\"Linear calib slope:     {b1:.3f}\")\n",
    "\n",
    "# 5b) fit logistic in log-odds space\n",
    "calib = LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000)\n",
    "eps   = 1e-15\n",
    "p_clip = np.clip(y_pred_prob_rf, eps, 1-eps)\n",
    "logodds = logit(p_clip).reshape(-1,1)\n",
    "calib.fit(logodds, y_test_validated)\n",
    "\n",
    "α = calib.intercept_[0]\n",
    "β = calib.coef_[0,0]\n",
    "print(f\"Calibration intercept (logit scale): {α:.3f}\")\n",
    "print(f\"Calibration slope     (logit scale): {β:.3f}\")\n",
    "\n",
    "# O : E ratio\n",
    "exp_pos = np.sum(y_pred_prob_rf)\n",
    "obs_pos = np.sum(y_test_validated)\n",
    "oe_ratio = obs_pos/exp_pos if exp_pos>0 else np.nan\n",
    "print(f\"O:E ratio (obs/exp): {oe_ratio:.3f}\")\n",
    "\n",
    "# Plot ROC and Precision-Recall curves\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test_validated, y_pred_prob_rf)\n",
    "precision_rf, recall_rf, _ = precision_recall_curve(y_test_validated, y_pred_prob_rf)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'ROC AUC = {roc_auc_rf:.3f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(recall_rf, precision_rf, label=f'Average Precision = {avg_precision_rf:.3f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "# Calibration\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(prob_pred, prob_true, marker='o', linestyle='-',\n",
    "         label='Calibration')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=0.8, label='Ideal')\n",
    "\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('Observed proportion')\n",
    "plt.title('Calibration curve')\n",
    "plt.text(0.05, 0.85,\n",
    "         f'Intercept = {α:.2f}\\nSlope = {β:.2f}',\n",
    "         transform=plt.gca().transAxes,\n",
    "         bbox=dict(boxstyle='round', alpha=0.1))\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37420bf-bd26-4e94-aac7-51b60231f408",
   "metadata": {},
   "source": [
    "## XGB Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d71794d-b719-4423-b2a9-73375d1624aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xgboost = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "\n",
    "X_train_raw      = X_train_raw.astype(np.float32)\n",
    "X_test_validated = X_test_validated.astype(np.float32)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "roc_auc_scores_xgb = cross_val_score(xgboost, X_train_raw, y_train_raw, cv=kfold, scoring='roc_auc')\n",
    "avg_precision_scores_xgb = cross_val_score(xgboost, X_train_raw, y_train_raw, cv=kfold, scoring='average_precision')\n",
    "\n",
    "# Train the model on the entire training set\n",
    "xgboost.fit(X_train_raw, y_train_raw)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob_xgb = xgboost.predict_proba(X_test_validated)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_xgb = roc_auc_score(y_test_validated, y_pred_prob_xgb)\n",
    "avg_precision_xgb = average_precision_score(y_test_validated, y_pred_prob_xgb)\n",
    "\n",
    "print(f\"ROC AUC (XGBoost): {roc_auc_xgb:.3f}\")\n",
    "print(f\"Average Precision (XGBoost): {avg_precision_xgb:.3f}\")\n",
    "print(f\"Cross-validated ROC AUC scores (XGBoost): {roc_auc_scores_xgb}\")\n",
    "print(f\"Cross-validated Average Precision scores (XGBoost): {avg_precision_scores_xgb}\")\n",
    "\n",
    "# Plot ROC and Precision-Recall curves\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test_validated, y_pred_prob_xgb)\n",
    "precision_xgb, recall_xgb, _ = precision_recall_curve(y_test_validated, y_pred_prob_xgb)\n",
    "\n",
    "# ─── Calibration curve (non-parametric bins) ────────────────────────────────\n",
    "prob_true, prob_pred = calibration_curve(\n",
    "    y_test_validated,\n",
    "    y_pred_prob_xgb,\n",
    "    n_bins=10,\n",
    "    strategy='uniform'\n",
    ")\n",
    "\n",
    "\n",
    "# 5b) fit logistic in log-odds space\n",
    "calib = LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000)\n",
    "eps   = 1e-15\n",
    "p_clip = np.clip(y_pred_prob_xgb, eps, 1-eps)\n",
    "logit_y_score = np.log(p_clip / (1 - p_clip))\n",
    "calib.fit(logit_y_score.reshape(-1, 1), y_test_validated)\n",
    "calib_intercept = calib.intercept_[0]\n",
    "calib_slope = calib.coef_[0][0]\n",
    "\n",
    "print(f\"Calibration intercept (logit scale): {calib_intercept:.3f}\")\n",
    "print(f\"Calibration slope     (logit scale): {calib_slope:.3f}\")\n",
    "\n",
    "# O : E ratio\n",
    "exp_pos = np.sum(y_pred_prob_xgb)\n",
    "obs_pos = np.sum(y_test_validated)\n",
    "oe_ratio = obs_pos/exp_pos if exp_pos>0 else np.nan\n",
    "print(f\"O:E ratio (obs/exp): {oe_ratio:.3f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(fpr_xgb, tpr_xgb, label=f'ROC AUC = {roc_auc_xgb:.3f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(recall_xgb, precision_xgb, label=f'Average Precision = {avg_precision_xgb:.3f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "# Calibration\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(prob_pred, prob_true, marker='o', linestyle='-',\n",
    "         label='Calibration')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=0.8, label='Ideal')\n",
    "\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('Observed proportion')\n",
    "plt.title('Calibration curve')\n",
    "plt.text(0.05, 0.85,\n",
    "         f'Intercept = {calib_intercept:.2f}\\nSlope = {calib_slope:.2f}',\n",
    "         transform=plt.gca().transAxes,\n",
    "         bbox=dict(boxstyle='round', alpha=0.1))\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c11cd-f3cd-4259-bf4e-d8960dab668b",
   "metadata": {},
   "source": [
    "## EBM Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f0fd6-a98c-4811-9f27-8372ba6ea680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "# Initialize the EBM model\n",
    "ebm = ExplainableBoostingClassifier(random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "roc_auc_scores_ebm = cross_val_score(ebm, X_train_raw, y_train_raw, cv=kfold, scoring='roc_auc')\n",
    "avg_precision_scores_ebm = cross_val_score(ebm, X_train_raw, y_train_raw, cv=kfold, scoring='average_precision')\n",
    "\n",
    "# Train the model on the entire training set\n",
    "ebm.fit(X_train_raw, y_train_raw)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob_ebm = ebm.predict_proba(X_test_validated)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_ebm = roc_auc_score(y_test_validated, y_pred_prob_ebm)\n",
    "avg_precision_ebm = average_precision_score(y_test_validated, y_pred_prob_ebm)\n",
    "\n",
    "print(f\"ROC AUC (EBM): {roc_auc_ebm:.3f}\")\n",
    "print(f\"Average Precision (EBM): {avg_precision_ebm:.3f}\")\n",
    "print(f\"Cross-validated ROC AUC scores (EBM): {roc_auc_scores_ebm}\")\n",
    "print(f\"Cross-validated Average Precision scores (EBM): {avg_precision_scores_ebm}\")\n",
    "\n",
    "# Plot ROC and Precision-Recall curves\n",
    "fpr_ebm, tpr_ebm, _ = roc_curve(y_test_validated, y_pred_prob_ebm)\n",
    "precision_ebm, recall_ebm, _ = precision_recall_curve(y_test_validated, y_pred_prob_ebm)\n",
    "\n",
    "# ─── Calibration curve (non-parametric bins) ────────────────────────────────\n",
    "prob_true, prob_pred = calibration_curve(\n",
    "    y_test_validated,\n",
    "    y_pred_prob_ebm,\n",
    "    n_bins=10,\n",
    "    strategy='uniform'\n",
    ")\n",
    "\n",
    "\n",
    "# 5b) fit logistic in log-odds space\n",
    "calib = LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000)\n",
    "eps   = 1e-15\n",
    "p_clip = np.clip(y_pred_prob_ebm, eps, 1-eps)\n",
    "logit_y_score = np.log(p_clip / (1 - p_clip))\n",
    "calib.fit(logit_y_score.reshape(-1, 1), y_test_validated)\n",
    "calib_intercept = calib.intercept_[0]\n",
    "calib_slope = calib.coef_[0][0]\n",
    "\n",
    "print(f\"Calibration intercept (logit scale): {calib_intercept:.3f}\")\n",
    "print(f\"Calibration slope     (logit scale): {calib_slope:.3f}\")\n",
    "\n",
    "# O : E ratio\n",
    "exp_pos = np.sum(y_pred_prob_ebm)\n",
    "obs_pos = np.sum(y_test_validated)\n",
    "oe_ratio = obs_pos/exp_pos if exp_pos>0 else np.nan\n",
    "print(f\"O:E ratio (obs/exp): {oe_ratio:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr_ebm, tpr_ebm, label=f'ROC AUC = {roc_auc_ebm:.3f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(recall_ebm, precision_ebm, label=f'Average Precision = {avg_precision_ebm:.3f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "# Calibration\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(prob_pred, prob_true, marker='o', linestyle='-',\n",
    "         label='Calibration')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=0.8, label='Ideal')\n",
    "\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('Observed proportion')\n",
    "plt.title('Calibration curve')\n",
    "plt.text(0.05, 0.85,\n",
    "         f'Intercept = {calib_intercept:.2f}\\nSlope = {calib_slope:.2f}',\n",
    "         transform=plt.gca().transAxes,\n",
    "         bbox=dict(boxstyle='round', alpha=0.1))\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcbc5b6-ce0c-4f86-9348-67d18dcdf63d",
   "metadata": {},
   "source": [
    "# Train on Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c991ae8-b189-4713-af5d-d4babc2418b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on validated data\n",
    "validated_mask = train_set['source'] == 'validated'\n",
    "X_train_validated = train_set[validated_mask].drop(columns=['ID', 'source'])\n",
    "y_train_validated = y[train_set[validated_mask].index]\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logistic_regression = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "roc_auc_scores = cross_val_score(logistic_regression, X_train_validated, y_train_validated, cv=kfold, scoring='roc_auc')\n",
    "avg_precision_scores = cross_val_score(logistic_regression, X_train_validated, y_train_validated, cv=kfold, scoring='average_precision')\n",
    "\n",
    "# Train the model on the entire training set\n",
    "logistic_regression.fit(X_train_validated, y_train_validated)\n",
    "\n",
    "# Predict on the validated test set\n",
    "y_pred_prob_validated = logistic_regression.predict_proba(X_test_validated)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_validated = roc_auc_score(y_test_validated, y_pred_prob_validated)\n",
    "avg_precision_validated = average_precision_score(y_test_validated, y_pred_prob_validated)\n",
    "\n",
    "print(f\"ROC AUC (Validated Data): {roc_auc_validated:.3f}\")\n",
    "print(f\"Average Precision (Validated Data): {avg_precision_validated:.3f}\")\n",
    "print(f\"Cross-validated ROC AUC scores (Raw Data): {roc_auc_scores}\")\n",
    "print(f\"Cross-validated Average Precision scores (Raw Data): {avg_precision_scores}\")\n",
    "\n",
    "# ─── Calibration curve ─────────────────────────────────────────────────────────\n",
    "prob_true, prob_pred = calibration_curve(y_test_validated,\n",
    "                                         y_pred_prob_validated,\n",
    "                                         n_bins=10, strategy='uniform')\n",
    "\n",
    "# ─── Calibration slope & intercept (Steyerberg approach) ───────────────────────\n",
    "eps = 1e-15                                                     # avoid log(0)\n",
    "logit_pred = np.log((y_pred_prob_validated + eps) /\n",
    "                    (1 - y_pred_prob_validated + eps)).reshape(-1, 1)\n",
    "\n",
    "cal_model = SKLogit(penalty=None, solver='lbfgs', max_iter=1000)\n",
    "cal_model.fit(logit_pred, y_test_validated)\n",
    "\n",
    "cal_intercept = cal_model.intercept_[0]\n",
    "cal_slope     = cal_model.coef_[0][0]\n",
    "\n",
    "print(f\"Calibration intercept: {cal_intercept:.3f}\")\n",
    "print(f\"Calibration slope:     {cal_slope:.3f}\")\n",
    "\n",
    "# Optional: Plot ROC and Precision-Recall curves\n",
    "fpr, tpr, _ = roc_curve(y_test_validated, y_pred_prob_validated)\n",
    "precision, recall, _ = precision_recall_curve(y_test_validated, y_pred_prob_validated)\n",
    "\n",
    "plt.figure(figsize=(13, 7), dpi=300)\n",
    "\n",
    "#plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(fpr, tpr, label=f'ROC AUC = {roc_auc_validated:.3f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=24)\n",
    "plt.ylabel('True Positive Rate', fontsize=24)\n",
    "plt.title('ROC Curve', fontsize=28)\n",
    "plt.legend(loc=\"lower right\", fontsize=20)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(recall, precision, label=f'Average Precision = {avg_precision_validated:.3f}')\n",
    "plt.xlabel('Recall', fontsize=24)\n",
    "plt.ylabel('Precision', fontsize=24)\n",
    "plt.title('Precision-Recall Curve', fontsize=28)\n",
    "plt.legend(loc=\"lower left\", fontsize=20)\n",
    "\n",
    "# Calibration\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(prob_pred, prob_true, marker='o', linestyle='-', label='Calibration')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=0.8, label='Ideal')\n",
    "plt.xlabel('Predicted probability'); plt.ylabel('Observed proportion')\n",
    "plt.title('Calibration curve')\n",
    "plt.text(0.05, 0.85,\n",
    "         f'Intercept = {cal_intercept:.2f}\\nSlope = {cal_slope:.2f}',\n",
    "         transform=plt.gca().transAxes,\n",
    "         bbox=dict(boxstyle='round', alpha=0.1))\n",
    "plt.legend(loc='lower right')\n",
    "#plt.savefig(\"val_roc_jbhi.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa50252-649d-4690-baf5-54fe171afe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gdm_prob = y_pred_prob_validated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21537aff-6d6f-410d-8857-793c99b5e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 7), dpi=300)\n",
    "\n",
    "#plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, label=f'ROC AUC = {roc_auc_validated:.3f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=24)\n",
    "plt.ylabel('True Positive Rate', fontsize=24)\n",
    "plt.title('ROC Curve', fontsize=28)\n",
    "plt.legend(loc=\"lower right\", fontsize=20)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(recall, precision, label=f'Average Precision = {avg_precision_validated:.3f}')\n",
    "plt.xlabel('Recall', fontsize=24)\n",
    "plt.ylabel('Precision', fontsize=24)\n",
    "plt.title('Precision-Recall Curve', fontsize=28)\n",
    "plt.legend(loc=\"lower left\", fontsize=20)\n",
    "\n",
    "#plt.savefig(\"raw_roc_jbhi.png\", dpi=300, bbox_inches='tight', transparent=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597932db-ed77-4df3-964a-eff68b0afe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 7), dpi=300)\n",
    "plt.plot(prob_pred, prob_true, marker='o', linestyle='-', label='Calibration')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=0.8, label='Ideal')\n",
    "plt.xlabel('Predicted probability'); plt.ylabel('Observed proportion')\n",
    "plt.title('Calibration curve')\n",
    "plt.text(0.05, 0.85,\n",
    "         f'Intercept = {cal_intercept:.2f}\\nSlope = {cal_slope:.2f}',\n",
    "         transform=plt.gca().transAxes,\n",
    "         bbox=dict(boxstyle='round', alpha=0.1))\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb14c1fc-7075-40c0-b2b1-c82ee2ea1147",
   "metadata": {},
   "source": [
    "## RF Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9b0b12-7611-4d0b-9fa1-2b156fb8c775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "roc_auc_scores_rf = cross_val_score(random_forest, X_train_validated, y_train_validated, cv=kfold, scoring='roc_auc')\n",
    "avg_precision_scores_rf = cross_val_score(random_forest, X_train_validated, y_train_validated, cv=kfold, scoring='average_precision')\n",
    "\n",
    "# Train the model on the entire training set\n",
    "random_forest.fit(X_train_validated, y_train_validated)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob_rf = random_forest.predict_proba(X_test_validated)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_rf = roc_auc_score(y_test_validated, y_pred_prob_rf)\n",
    "avg_precision_rf = average_precision_score(y_test_validated, y_pred_prob_rf)\n",
    "\n",
    "print(f\"ROC AUC (Random Forest): {roc_auc_rf:.3f}\")\n",
    "print(f\"Average Precision (Random Forest): {avg_precision_rf:.3f}\")\n",
    "print(f\"Cross-validated ROC AUC scores (Random Forest): {roc_auc_scores_rf}\")\n",
    "print(f\"Cross-validated Average Precision scores (Random Forest): {avg_precision_scores_rf}\")\n",
    "\n",
    "\n",
    "# ─── Calibration curve (non-parametric bins) ────────────────────────────────\n",
    "prob_true, prob_pred = calibration_curve(\n",
    "    y_test_validated,\n",
    "    y_pred_prob_rf,\n",
    "    n_bins=10,\n",
    "    strategy='uniform'\n",
    ")\n",
    "\n",
    "# linear least-squares in prob space\n",
    "lr = LinearRegression().fit(prob_pred.reshape(-1,1), prob_true)\n",
    "b0, b1 = lr.intercept_, lr.coef_[0]\n",
    "print(f\"Linear calib intercept: {b0:.3f}\")\n",
    "print(f\"Linear calib slope:     {b1:.3f}\")\n",
    "\n",
    "# 5b) fit logistic in log-odds space\n",
    "calib = LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000)\n",
    "eps   = 1e-15\n",
    "p_clip = np.clip(y_pred_prob_rf, eps, 1-eps)\n",
    "logodds = logit(p_clip).reshape(-1,1)\n",
    "calib.fit(logodds, y_test_validated)\n",
    "\n",
    "α = calib.intercept_[0]\n",
    "β = calib.coef_[0,0]\n",
    "print(f\"Calibration intercept (logit scale): {α:.3f}\")\n",
    "print(f\"Calibration slope     (logit scale): {β:.3f}\")\n",
    "\n",
    "# O : E ratio\n",
    "exp_pos = np.sum(y_pred_prob_rf)\n",
    "obs_pos = np.sum(y_test_validated)\n",
    "oe_ratio = obs_pos/exp_pos if exp_pos>0 else np.nan\n",
    "print(f\"O:E ratio (obs/exp): {oe_ratio:.3f}\")\n",
    "\n",
    "# Plot ROC and Precision-Recall curves\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test_validated, y_pred_prob_rf)\n",
    "precision_rf, recall_rf, _ = precision_recall_curve(y_test_validated, y_pred_prob_rf)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'ROC AUC = {roc_auc_rf:.3f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(recall_rf, precision_rf, label=f'Average Precision = {avg_precision_rf:.3f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "# Calibration\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(prob_pred, prob_true, marker='o', linestyle='-',\n",
    "         label='Calibration')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=0.8, label='Ideal')\n",
    "\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('Observed proportion')\n",
    "plt.title('Calibration curve')\n",
    "plt.text(0.05, 0.85,\n",
    "         f'Intercept = {α:.2f}\\nSlope = {β:.2f}',\n",
    "         transform=plt.gca().transAxes,\n",
    "         bbox=dict(boxstyle='round', alpha=0.1))\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0990103-e17c-4243-a002-0443e645e429",
   "metadata": {},
   "source": [
    "## XGB Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef50aa0-dbfe-46ba-8196-69b6337982a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert all columns in X_train_raw from object to float\n",
    "X_train_validated = X_train_validated.astype(float)\n",
    "\n",
    "xgboost = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "roc_auc_scores_xgb = cross_val_score(xgboost, X_train_validated, y_train_validated, cv=kfold, scoring='roc_auc')\n",
    "avg_precision_scores_xgb = cross_val_score(xgboost, X_train_validated, y_train_validated, cv=kfold, scoring='average_precision')\n",
    "\n",
    "# Train the model on the entire training set\n",
    "xgboost.fit(X_train_validated, y_train_validated)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob_xgb = xgboost.predict_proba(X_test_validated)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_xgb = roc_auc_score(y_test_validated, y_pred_prob_xgb)\n",
    "avg_precision_xgb = average_precision_score(y_test_validated, y_pred_prob_xgb)\n",
    "\n",
    "print(f\"ROC AUC (XGBoost): {roc_auc_xgb:.3f}\")\n",
    "print(f\"Average Precision (XGBoost): {avg_precision_xgb:.3f}\")\n",
    "print(f\"Cross-validated ROC AUC scores (XGBoost): {roc_auc_scores_xgb}\")\n",
    "print(f\"Cross-validated Average Precision scores (XGBoost): {avg_precision_scores_xgb}\")\n",
    "\n",
    "# Plot ROC and Precision-Recall curves\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test_validated, y_pred_prob_xgb)\n",
    "precision_xgb, recall_xgb, _ = precision_recall_curve(y_test_validated, y_pred_prob_xgb)\n",
    "\n",
    "# ─── Calibration curve (non-parametric bins) ────────────────────────────────\n",
    "prob_true, prob_pred = calibration_curve(\n",
    "    y_test_validated,\n",
    "    y_pred_prob_xgb,\n",
    "    n_bins=10,\n",
    "    strategy='uniform'\n",
    ")\n",
    "\n",
    "\n",
    "# 5b) fit logistic in log-odds space\n",
    "calib = LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000)\n",
    "eps   = 1e-15\n",
    "p_clip = np.clip(y_pred_prob_xgb, eps, 1-eps)\n",
    "logit_y_score = np.log(p_clip / (1 - p_clip))\n",
    "calib.fit(logit_y_score.reshape(-1, 1), y_test_validated)\n",
    "calib_intercept = calib.intercept_[0]\n",
    "calib_slope = calib.coef_[0][0]\n",
    "\n",
    "print(f\"Calibration intercept (logit scale): {calib_intercept:.3f}\")\n",
    "print(f\"Calibration slope     (logit scale): {calib_slope:.3f}\")\n",
    "\n",
    "# O : E ratio\n",
    "exp_pos = np.sum(y_pred_prob_xgb)\n",
    "obs_pos = np.sum(y_test_validated)\n",
    "oe_ratio = obs_pos/exp_pos if exp_pos>0 else np.nan\n",
    "print(f\"O:E ratio (obs/exp): {oe_ratio:.3f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(fpr_xgb, tpr_xgb, label=f'ROC AUC = {roc_auc_xgb:.3f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(recall_xgb, precision_xgb, label=f'Average Precision = {avg_precision_xgb:.3f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "# Calibration\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(prob_pred, prob_true, marker='o', linestyle='-',\n",
    "         label='Calibration')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=0.8, label='Ideal')\n",
    "\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('Observed proportion')\n",
    "plt.title('Calibration curve')\n",
    "plt.text(0.05, 0.85,\n",
    "         f'Intercept = {calib_intercept:.2f}\\nSlope = {calib_slope:.2f}',\n",
    "         transform=plt.gca().transAxes,\n",
    "         bbox=dict(boxstyle='round', alpha=0.1))\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6252dca-5fe4-4002-ae59-6c82ddadc7f8",
   "metadata": {},
   "source": [
    "## EBM Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dae6ddc-3e5d-4a9b-b11e-c17eec819a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "# Initialize the EBM model\n",
    "ebm = ExplainableBoostingClassifier(random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "roc_auc_scores_ebm = cross_val_score(ebm, X_train_validated, y_train_validated, cv=kfold, scoring='roc_auc')\n",
    "avg_precision_scores_ebm = cross_val_score(ebm, X_train_validated, y_train_validated, cv=kfold, scoring='average_precision')\n",
    "\n",
    "# Train the model on the entire training set\n",
    "ebm.fit(X_train_validated, y_train_validated)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob_ebm = ebm.predict_proba(X_test_validated)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_ebm = roc_auc_score(y_test_validated, y_pred_prob_ebm)\n",
    "avg_precision_ebm = average_precision_score(y_test_validated, y_pred_prob_ebm)\n",
    "\n",
    "print(f\"ROC AUC (EBM): {roc_auc_ebm:.3f}\")\n",
    "print(f\"Average Precision (EBM): {avg_precision_ebm:.3f}\")\n",
    "print(f\"Cross-validated ROC AUC scores (EBM): {roc_auc_scores_ebm}\")\n",
    "print(f\"Cross-validated Average Precision scores (EBM): {avg_precision_scores_ebm}\")\n",
    "\n",
    "# Plot ROC and Precision-Recall curves\n",
    "fpr_ebm, tpr_ebm, _ = roc_curve(y_test_validated, y_pred_prob_ebm)\n",
    "precision_ebm, recall_ebm, _ = precision_recall_curve(y_test_validated, y_pred_prob_ebm)\n",
    "\n",
    "# ─── Calibration curve (non-parametric bins) ────────────────────────────────\n",
    "prob_true, prob_pred = calibration_curve(\n",
    "    y_test_validated,\n",
    "    y_pred_prob_ebm,\n",
    "    n_bins=10,\n",
    "    strategy='uniform'\n",
    ")\n",
    "\n",
    "\n",
    "# 5b) fit logistic in log-odds space\n",
    "calib = LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000)\n",
    "eps   = 1e-15\n",
    "p_clip = np.clip(y_pred_prob_ebm, eps, 1-eps)\n",
    "logit_y_score = np.log(p_clip / (1 - p_clip))\n",
    "calib.fit(logit_y_score.reshape(-1, 1), y_test_validated)\n",
    "calib_intercept = calib.intercept_[0]\n",
    "calib_slope = calib.coef_[0][0]\n",
    "\n",
    "print(f\"Calibration intercept (logit scale): {calib_intercept:.3f}\")\n",
    "print(f\"Calibration slope     (logit scale): {calib_slope:.3f}\")\n",
    "\n",
    "# O : E ratio\n",
    "exp_pos = np.sum(y_pred_prob_ebm)\n",
    "obs_pos = np.sum(y_test_validated)\n",
    "oe_ratio = obs_pos/exp_pos if exp_pos>0 else np.nan\n",
    "print(f\"O:E ratio (obs/exp): {oe_ratio:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(fpr_ebm, tpr_ebm, label=f'ROC AUC = {roc_auc_ebm:.3f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(recall_ebm, precision_ebm, label=f'Average Precision = {avg_precision_ebm:.3f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "# Calibration\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(prob_pred, prob_true, marker='o', linestyle='-',\n",
    "         label='Calibration')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=0.8, label='Ideal')\n",
    "\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('Observed proportion')\n",
    "plt.title('Calibration curve')\n",
    "plt.text(0.05, 0.85,\n",
    "         f'Intercept = {calib_intercept:.2f}\\nSlope = {calib_slope:.2f}',\n",
    "         transform=plt.gca().transAxes,\n",
    "         bbox=dict(boxstyle='round', alpha=0.1))\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14481f0-23b8-4865-8aea-2bb02e63a991",
   "metadata": {},
   "source": [
    "## Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f92816-5347-436a-9b44-76026b09dd96",
   "metadata": {},
   "source": [
    "### ROC & AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc860a44-5386-441c-baa9-6701e4a55382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "# Compute ROC metrics\n",
    "fpr_ehr, tpr_ehr, _ = roc_curve(val_gdm_true, ehr_gdm_prob)\n",
    "fpr_val, tpr_val, _ = roc_curve(val_gdm_true, val_gdm_prob)\n",
    "auc_ehr = roc_auc_score(val_gdm_true, ehr_gdm_prob)\n",
    "auc_val = roc_auc_score(val_gdm_true, val_gdm_prob)\n",
    "\n",
    "# Compute PR metrics\n",
    "prec_ehr, rec_ehr, _ = precision_recall_curve(val_gdm_true, ehr_gdm_prob)\n",
    "prec_val, rec_val, _ = precision_recall_curve(val_gdm_true, val_gdm_prob)\n",
    "ap_ehr = average_precision_score(val_gdm_true, ehr_gdm_prob)\n",
    "ap_val = average_precision_score(val_gdm_true, val_gdm_prob)\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size']   = 16\n",
    "\n",
    "# Plot\n",
    "fig, (ax_roc, ax_pr) = plt.subplots(1, 2, figsize=(12, 5), dpi=300)\n",
    "\n",
    "# ROC plot\n",
    "ax_roc.plot(fpr_ehr, tpr_ehr, label=f'EHR-GDM (AUC = {auc_ehr:.3f})')\n",
    "ax_roc.plot(fpr_val, tpr_val, label=f'VAL-GDM (AUC = {auc_val:.3f})')\n",
    "ax_roc.plot([0,1], [0,1], '--', color='grey')\n",
    "ax_roc.set_xlabel('False Positive Rate')\n",
    "ax_roc.set_ylabel('True Positive Rate')\n",
    "ax_roc.set_title('ROC Curves')\n",
    "ax_roc.legend(loc='lower right', fontsize=14)\n",
    "\n",
    "# PR plot\n",
    "ax_pr.step(rec_ehr, prec_ehr, where='post', label=f'EHR-GDM (AP = {ap_ehr:.3f})')\n",
    "ax_pr.step(rec_val, prec_val, where='post', label=f'VAL-GDM (AP = {ap_val:.3f})')\n",
    "ax_pr.set_xlabel('Recall')\n",
    "ax_pr.set_ylabel('Precision')\n",
    "ax_pr.set_title('Precision–Recall Curves')\n",
    "ax_pr.set_xlim(0, 1)\n",
    "ax_pr.set_ylim(0, 1.05)\n",
    "ax_pr.legend(loc='lower left', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"roc_pr_comparison_jmir.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48d3e5-46f3-4d27-9cc8-47cde8e74132",
   "metadata": {},
   "source": [
    "### Calibration Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11bea8-8957-4905-ac2b-22e88664d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute calibration for EHR-GDM\n",
    "prob_true_ehr, prob_pred_ehr = calibration_curve(\n",
    "    val_gdm_true,      # y_test_validated\n",
    "    ehr_gdm_prob,      # y_pred_prob_validated for EHR\n",
    "    n_bins=10,\n",
    "    strategy='uniform'\n",
    ")\n",
    "\n",
    "# Compute calibration for VAL-GDM\n",
    "prob_true_val, prob_pred_val = calibration_curve(\n",
    "    val_gdm_true,      # same ground truth\n",
    "    val_gdm_prob,      # y_pred_prob_validated for VAL\n",
    "    n_bins=10,\n",
    "    strategy='uniform'\n",
    ")\n",
    "# Plot\n",
    "fig, (ax_ehr, ax_val) = plt.subplots(1, 2, figsize=(12, 5), dpi=300)\n",
    "\n",
    "# Left: EHR calibration\n",
    "ax_ehr.plot(prob_pred_ehr, prob_true_ehr, marker='o', label='EHR-GDM')\n",
    "ax_ehr.plot([0,1],[0,1],'--', color='grey')\n",
    "ax_ehr.set_xlabel('Mean Predicted Probability')\n",
    "ax_ehr.set_ylabel('Observed Proportion')\n",
    "ax_ehr.set_title('Calibration Curve for EHR-GDM')\n",
    "ax_ehr.legend(loc='lower right')\n",
    "\n",
    "# Right: VAL calibration\n",
    "ax_val.plot(prob_pred_val, prob_true_val, marker='o', label='VAL-GDM')\n",
    "ax_val.plot([0,1],[0,1],'--', color='grey')\n",
    "ax_val.set_xlabel('Mean Predicted Probability')\n",
    "ax_val.set_ylabel('Observed Proportion')\n",
    "ax_val.set_title('Calibration Curve for VAL-GDM')\n",
    "ax_val.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"calibration_jmir.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2626e9a1-4f3a-4a1c-b068-39e896ac3116",
   "metadata": {},
   "source": [
    "# Label Noise Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e80281-1bd5-4990-81aa-a6796819d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = merged_df.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = merged_df.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# Remove target and identifier columns from features\n",
    "target_column = 'GDM'\n",
    "numerical_features.remove(target_column)\n",
    "numerical_features.remove('ID')\n",
    "\n",
    "# Preprocessor setup\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features),\n",
    "        ('num', StandardScaler(), numerical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Apply preprocessing to the entire dataset except 'GDM' and 'ID'\n",
    "X = merged_df.drop(columns=[target_column, 'ID'])\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "y = merged_df[target_column].values\n",
    "\n",
    "# Convert processed features back to DataFrame with transformed column names\n",
    "columns_transformed = (list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))\n",
    "                       + numerical_features)\n",
    "X_processed_df = pd.DataFrame(X_processed, columns=columns_transformed)\n",
    "X_processed_df['ID'] = merged_df['ID'].values\n",
    "\n",
    "# Initial group-based split to create train and test sets\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.7, random_state=42)\n",
    "train_idxs, test_idxs = next(gss.split(X_processed_df, groups=X_processed_df['ID']))\n",
    "\n",
    "train_set = X_processed_df.iloc[train_idxs]\n",
    "test_set = X_processed_df.iloc[test_idxs]\n",
    "y_train = y[train_idxs]\n",
    "y_test = y[test_idxs]\n",
    "\n",
    "# Separate features from 'ID' for training and test sets\n",
    "X_train = train_set.drop(columns=['ID'])\n",
    "X_test = test_set.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af897f5d-94d2-44b1-b673-77706008fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to Add Incremental Label Noise, ensuring only original labels are flipped\n",
    "def add_incremental_label_noise(y, pos_noise_level, neg_noise_level):\n",
    "    \"\"\"\n",
    "    Add incremental label noise to a binary target array, ensuring that only original labels are flipped.\n",
    "    \n",
    "    Parameters:\n",
    "    y (np.array): The original labels.\n",
    "    pos_noise_level (float): The proportion of positive labels (1s) to be flipped to 0s.\n",
    "    neg_noise_level (float): The proportion of negative labels (0s) to be flipped to 1s.\n",
    "    \n",
    "    Returns:\n",
    "    np.array: The labels with noise introduced.\n",
    "    \"\"\"\n",
    "    # Make a copy of the original labels\n",
    "    y_noisy = y.copy()\n",
    "    \n",
    "    # Identify original positive and negative indices\n",
    "    original_pos_indices = np.where(y == 1)[0]\n",
    "    original_neg_indices = np.where(y == 0)[0]\n",
    "    \n",
    "    # Calculate the number of labels to flip\n",
    "    n_pos_noisy = int(pos_noise_level * len(original_pos_indices))\n",
    "    n_neg_noisy = int(neg_noise_level * len(original_neg_indices))\n",
    "    \n",
    "    # Select indices to flip\n",
    "    pos_noisy_indices = np.random.choice(original_pos_indices, n_pos_noisy, replace=False)\n",
    "    neg_noisy_indices = np.random.choice(original_neg_indices, n_neg_noisy, replace=False)\n",
    "    \n",
    "    # Flip the selected original labels only\n",
    "    y_noisy[pos_noisy_indices] = 0\n",
    "    y_noisy[neg_noisy_indices] = 1\n",
    "    \n",
    "    return y_noisy\n",
    "\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logistic_regression = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Different levels of noise to evaluate\n",
    "noise_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]  # Example noise levels\n",
    "\n",
    "# Evaluate model performance with different noise levels\n",
    "performance_metrics_incremental = []\n",
    "\n",
    "# Evaluate model performance with incremental label noise\n",
    "for pos_noise_level in noise_levels:\n",
    "    for neg_noise_level in noise_levels:\n",
    "        print(f\"Evaluating pos_noise_level: {pos_noise_level}, neg_noise_level: {neg_noise_level}\")\n",
    "        \n",
    "        # Add incremental label noise to the training data\n",
    "        y_train_noisy = add_incremental_label_noise(y_train, pos_noise_level, neg_noise_level)\n",
    "\n",
    "        # Train the model on noisy training data\n",
    "        logistic_regression.fit(X_train, y_train_noisy)\n",
    "\n",
    "        # Predict on the clean test set\n",
    "        y_pred_proba = logistic_regression.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Evaluate performance on the clean test set\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        ap_score = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "        # Store metrics as dictionaries\n",
    "        metrics = {\n",
    "            'pos_noise_level': pos_noise_level,\n",
    "            'neg_noise_level': neg_noise_level,\n",
    "            'roc_auc_mean': roc_auc,\n",
    "            'average_precision_mean': ap_score,\n",
    "        }\n",
    "        performance_metrics_incremental.append(metrics)\n",
    "\n",
    "# Display AUC and AP for each noise level combination\n",
    "for metrics in performance_metrics_incremental:\n",
    "    pos_noise_level_train = metrics['pos_noise_level']\n",
    "    neg_noise_level_train = metrics['neg_noise_level']\n",
    "    roc_auc_mean_train = metrics['roc_auc_mean']\n",
    "    average_precision_mean_train = metrics['average_precision_mean']\n",
    "    \n",
    "    print(f\"Pos Noise Level: {pos_noise_level_train*100:.0f}%, Neg Noise Level: {neg_noise_level_train*100:.0f}%\")\n",
    "    print(f\"  ROC AUC Mean: {roc_auc_mean_train:.3f}\")\n",
    "    print(f\"  Average Precision Mean: {average_precision_mean_train:.3f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Plot ROC AUC and Average Precision for different noise levels with incremental label noise\n",
    "# Extract values for plotting\n",
    "pos_noise_levels_train = sorted(list(set([metrics['pos_noise_level'] for metrics in performance_metrics_incremental])))\n",
    "neg_noise_levels_train = sorted(list(set([metrics['neg_noise_level'] for metrics in performance_metrics_incremental])))\n",
    "\n",
    "roc_auc_matrix_train = np.zeros((len(pos_noise_levels_train), len(neg_noise_levels_train)))\n",
    "ap_matrix_train = np.zeros((len(pos_noise_levels_train), len(neg_noise_levels_train)))\n",
    "\n",
    "for metrics in performance_metrics_incremental:\n",
    "    pos_index_train = pos_noise_levels_train.index(metrics['pos_noise_level'])\n",
    "    neg_index_train = neg_noise_levels_train.index(metrics['neg_noise_level'])\n",
    "    roc_auc_matrix_train[pos_index_train, neg_index_train] = metrics['roc_auc_mean']\n",
    "    ap_matrix_train[pos_index_train, neg_index_train] = metrics['average_precision_mean']\n",
    "\n",
    "# Plot ROC AUC\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(roc_auc_matrix_train, interpolation='nearest', cmap='viridis')\n",
    "plt.title('ROC AUC vs Noise Levels')\n",
    "plt.xlabel('Neg Noise Level')\n",
    "plt.ylabel('Pos Noise Level')\n",
    "plt.colorbar(label='ROC AUC')\n",
    "plt.xticks(ticks=np.arange(len(neg_noise_levels_train)), labels=[f'{nl*100:.0f}%' for nl in neg_noise_levels_train])\n",
    "plt.yticks(ticks=np.arange(len(pos_noise_levels_train)), labels=[f'{pl*100:.0f}%' for pl in pos_noise_levels_train])\n",
    "\n",
    "# Annotate ROC AUC values\n",
    "for i in range(len(pos_noise_levels_train)):\n",
    "    for j in range(len(neg_noise_levels_train)):\n",
    "        plt.text(j, i, f'{roc_auc_matrix_train[i, j]:.2f}', ha='center', va='center', color='white')\n",
    "\n",
    "# Plot Average Precision\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(ap_matrix_train, interpolation='nearest', cmap='viridis')\n",
    "plt.title('Average Precision vs Noise Levels')\n",
    "plt.xlabel('Neg Noise Level')\n",
    "plt.ylabel('Pos Noise Level')\n",
    "plt.colorbar(label='Average Precision')\n",
    "plt.xticks(ticks=np.arange(len(neg_noise_levels_train)), labels=[f'{nl*100:.0f}%' for nl in neg_noise_levels_train])\n",
    "plt.yticks(ticks=np.arange(len(pos_noise_levels_train)), labels=[f'{pl*100:.0f}%' for pl in pos_noise_levels_train])\n",
    "\n",
    "# Annotate AP values\n",
    "for i in range(len(pos_noise_levels_train)):\n",
    "    for j in range(len(neg_noise_levels_train)):\n",
    "        plt.text(j, i, f'{ap_matrix_train[i, j]:.2f}', ha='center', va='center', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e171a3d-1c4d-4c63-9dc7-bf1de15518b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC AUC\n",
    "plt.figure(figsize=(14, 6), dpi=300)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(roc_auc_matrix_train, interpolation='nearest', cmap='cividis')\n",
    "plt.title('Increasing Noise in Train Set: \\nROC AUC', fontsize=20)\n",
    "plt.xlabel('Increasing False Positives', fontsize=18)\n",
    "plt.ylabel('Increasing False Negatives', fontsize=18)\n",
    "plt.colorbar(label='ROC AUC')\n",
    "plt.xticks(ticks=np.arange(len(neg_noise_levels_train)), labels=[f'{nl*100:.0f}%' for nl in neg_noise_levels_train], fontsize=12)\n",
    "plt.yticks(ticks=np.arange(len(pos_noise_levels_train)), labels=[f'{pl*100:.0f}%' for pl in pos_noise_levels_train], fontsize=12)\n",
    "\n",
    "# Annotate ROC AUC values with contrasting color based on background intensity\n",
    "for i in range(len(pos_noise_levels_train)):\n",
    "    for j in range(len(neg_noise_levels_train)):\n",
    "        value = roc_auc_matrix_train[i, j]\n",
    "        color = 'midnightblue' if value > 0.7 else 'white'  # Choose color based on a threshold\n",
    "        plt.text(j, i, f'{value:.2f}', ha='center', va='center', color=color, fontsize=10)\n",
    "\n",
    "# Plot Average Precision\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(ap_matrix_train, interpolation='nearest', cmap='cividis')\n",
    "plt.title('Increasing Noise in Train Set: \\nAverage Precision', fontsize=20)\n",
    "plt.xlabel('Increasing False Positives', fontsize=18)\n",
    "plt.ylabel('Increasing False Negatives', fontsize=18)\n",
    "plt.colorbar(label='Average Precision')\n",
    "plt.xticks(ticks=np.arange(len(neg_noise_levels_train)), labels=[f'{nl*100:.0f}%' for nl in neg_noise_levels_train], fontsize=12)\n",
    "plt.yticks(ticks=np.arange(len(pos_noise_levels_train)), labels=[f'{pl*100:.0f}%' for pl in pos_noise_levels_train], fontsize=12)\n",
    "\n",
    "# Annotate AP values with contrasting color based on background intensity\n",
    "for i in range(len(pos_noise_levels_train)):\n",
    "    for j in range(len(neg_noise_levels_train)):\n",
    "        value = ap_matrix_train[i, j]\n",
    "        color = 'midnightblue' if value > 0.4 else 'white'  # Choose color based on threshold\n",
    "        plt.text(j, i, f'{value:.2f}', ha='center', va='center', color=color, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c69a3b-ace9-4f11-b5e5-b417d024f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC AUC\n",
    "plt.figure(figsize=(14, 6), dpi=500)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(roc_auc_matrix, interpolation='nearest', cmap='cividis')\n",
    "plt.title('ROC AUC vs Noise Levels', fontsize=20)\n",
    "plt.xlabel('Neg Noise Level', fontsize=18)\n",
    "plt.ylabel('Pos Noise Level', fontsize=18)\n",
    "plt.colorbar(label='ROC AUC')\n",
    "plt.xticks(ticks=np.arange(len(neg_noise_levels)), labels=[f'{nl*100:.0f}%' for nl in neg_noise_levels], fontsize=12)\n",
    "plt.yticks(ticks=np.arange(len(pos_noise_levels)), labels=[f'{pl*100:.0f}%' for pl in pos_noise_levels], fontsize=12)\n",
    "\n",
    "# Annotate ROC AUC values with contrasting color based on background intensity\n",
    "for i in range(len(pos_noise_levels)):\n",
    "    for j in range(len(neg_noise_levels)):\n",
    "        value = roc_auc_matrix[i, j]\n",
    "        color = 'midnightblue' if value > 0.7 else 'white'  # Choose color based on a threshold\n",
    "        plt.text(j, i, f'{value:.2f}', ha='center', va='center', color=color, fontsize=10)\n",
    "\n",
    "# Plot Average Precision\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(ap_matrix, interpolation='nearest', cmap='cividis')\n",
    "plt.title('Average Precision vs Noise Levels', fontsize=20)\n",
    "plt.xlabel('Neg Noise Level', fontsize=18)\n",
    "plt.ylabel('Pos Noise Level', fontsize=18)\n",
    "plt.colorbar(label='Average Precision')\n",
    "plt.xticks(ticks=np.arange(len(neg_noise_levels)), labels=[f'{nl*100:.0f}%' for nl in neg_noise_levels], fontsize=12)\n",
    "plt.yticks(ticks=np.arange(len(pos_noise_levels)), labels=[f'{pl*100:.0f}%' for pl in pos_noise_levels], fontsize=12)\n",
    "\n",
    "# Annotate AP values with contrasting color based on background intensity\n",
    "for i in range(len(pos_noise_levels)):\n",
    "    for j in range(len(neg_noise_levels)):\n",
    "        value = ap_matrix[i, j]\n",
    "        color = 'midnightblue' if value > 0.4 else 'white'  # Choose color based on threshold\n",
    "        plt.text(j, i, f'{value:.2f}', ha='center', va='center', color=color, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e9174d-4aa8-4f0c-b25c-3710af6b486b",
   "metadata": {},
   "source": [
    "# Label Noise Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd40b7-a919-44b4-bd83-6c69ff773db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to Add Incremental Label Noise, ensuring only original labels are flipped\n",
    "def add_incremental_label_noise(y, pos_noise_level, neg_noise_level):\n",
    "    \"\"\"\n",
    "    Add incremental label noise to a binary target array, ensuring that only original labels are flipped.\n",
    "    \n",
    "    Parameters:\n",
    "    y (np.array): The original labels.\n",
    "    pos_noise_level (float): The proportion of positive labels (1s) to be flipped to 0s.\n",
    "    neg_noise_level (float): The proportion of negative labels (0s) to be flipped to 1s.\n",
    "    \n",
    "    Returns:\n",
    "    np.array: The labels with noise introduced.\n",
    "    \"\"\"\n",
    "    # Make a copy of the original labels\n",
    "    y_noisy = y.copy()\n",
    "    \n",
    "    # Identify original positive and negative indices\n",
    "    original_pos_indices = np.where(y == 1)[0]\n",
    "    original_neg_indices = np.where(y == 0)[0]\n",
    "    \n",
    "    # Calculate the number of labels to flip\n",
    "    n_pos_noisy = int(pos_noise_level * len(original_pos_indices))\n",
    "    n_neg_noisy = int(neg_noise_level * len(original_neg_indices))\n",
    "    \n",
    "    # Select indices to flip\n",
    "    pos_noisy_indices = np.random.choice(original_pos_indices, n_pos_noisy, replace=False)\n",
    "    neg_noisy_indices = np.random.choice(original_neg_indices, n_neg_noisy, replace=False)\n",
    "    \n",
    "    # Flip the selected original labels only\n",
    "    y_noisy[pos_noisy_indices] = 0\n",
    "    y_noisy[neg_noisy_indices] = 1\n",
    "    \n",
    "    return y_noisy\n",
    "\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logistic_regression = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Different levels of noise to evaluate\n",
    "noise_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]  # Example noise levels\n",
    "\n",
    "\n",
    "# Evaluate model performance with different noise levels\n",
    "performance_metrics_incremental = []\n",
    "\n",
    "# Evaluate model performance with incremental label noise\n",
    "for pos_noise_level in noise_levels:\n",
    "    for neg_noise_level in noise_levels:\n",
    "        print(f\"Evaluating pos_noise_level: {pos_noise_level}, neg_noise_level: {neg_noise_level}\")\n",
    "        \n",
    "        # Add incremental label noise to the training data\n",
    "        y_test_noisy = add_incremental_label_noise(y_test, pos_noise_level, neg_noise_level)\n",
    "\n",
    "        # Train the model\n",
    "        logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the validation set\n",
    "        y_pred_proba = logistic_regression.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Evaluate performance\n",
    "        roc_auc = roc_auc_score(y_test_noisy, y_pred_proba)\n",
    "        ap_score = average_precision_score(y_test_noisy, y_pred_proba)\n",
    "\n",
    "        # Store metrics as dictionaries\n",
    "        metrics = {\n",
    "            'pos_noise_level': pos_noise_level,\n",
    "            'neg_noise_level': neg_noise_level,\n",
    "            'roc_auc_mean': roc_auc,\n",
    "            'average_precision_mean': ap_score,\n",
    "        }\n",
    "        performance_metrics_incremental.append(metrics)\n",
    "\n",
    "# Display AUC and AP for each noise level combination\n",
    "for metrics in performance_metrics_incremental:\n",
    "    pos_noise_level = metrics['pos_noise_level']\n",
    "    neg_noise_level = metrics['neg_noise_level']\n",
    "    roc_auc_mean = metrics['roc_auc_mean']\n",
    "    average_precision_mean = metrics['average_precision_mean']\n",
    "    \n",
    "    print(f\"Pos Noise Level: {pos_noise_level*100:.0f}%, Neg Noise Level: {neg_noise_level*100:.0f}%\")\n",
    "    print(f\"  ROC AUC Mean: {roc_auc_mean:.3f}\")\n",
    "    print(f\"  Average Precision Mean: {average_precision_mean:.3f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Plot ROC AUC and Average Precision for different noise levels with incremental label noise\n",
    "# Extract values for plotting\n",
    "pos_noise_levels = sorted(list(set([metrics['pos_noise_level'] for metrics in performance_metrics_incremental])))\n",
    "neg_noise_levels = sorted(list(set([metrics['neg_noise_level'] for metrics in performance_metrics_incremental])))\n",
    "\n",
    "roc_auc_matrix = np.zeros((len(pos_noise_levels), len(neg_noise_levels)))\n",
    "ap_matrix = np.zeros((len(pos_noise_levels), len(neg_noise_levels)))\n",
    "\n",
    "for metrics in performance_metrics_incremental:\n",
    "    pos_index = pos_noise_levels.index(metrics['pos_noise_level'])\n",
    "    neg_index = neg_noise_levels.index(metrics['neg_noise_level'])\n",
    "    roc_auc_matrix[pos_index, neg_index] = metrics['roc_auc_mean']\n",
    "    ap_matrix[pos_index, neg_index] = metrics['average_precision_mean']\n",
    "\n",
    "# Plot ROC AUC\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(roc_auc_matrix, interpolation='nearest', cmap='viridis')\n",
    "plt.title('ROC AUC vs Noise Levels')\n",
    "plt.xlabel('Neg Noise Level')\n",
    "plt.ylabel('Pos Noise Level')\n",
    "plt.colorbar(label='ROC AUC')\n",
    "plt.xticks(ticks=np.arange(len(neg_noise_levels)), labels=[f'{nl*100:.0f}%' for nl in neg_noise_levels])\n",
    "plt.yticks(ticks=np.arange(len(pos_noise_levels)), labels=[f'{pl*100:.0f}%' for pl in pos_noise_levels])\n",
    "\n",
    "# Annotate ROC AUC values\n",
    "for i in range(len(pos_noise_levels)):\n",
    "    for j in range(len(neg_noise_levels)):\n",
    "        plt.text(j, i, f'{roc_auc_matrix[i, j]:.2f}', ha='center', va='center', color='white')\n",
    "\n",
    "# Plot Average Precision\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(ap_matrix, interpolation='nearest', cmap='viridis')\n",
    "plt.title('Average Precision vs Noise Levels')\n",
    "plt.xlabel('Neg Noise Level')\n",
    "plt.ylabel('Pos Noise Level')\n",
    "plt.colorbar(label='Average Precision')\n",
    "plt.xticks(ticks=np.arange(len(neg_noise_levels)), labels=[f'{nl*100:.0f}%' for nl in neg_noise_levels])\n",
    "plt.yticks(ticks=np.arange(len(pos_noise_levels)), labels=[f'{pl*100:.0f}%' for pl in pos_noise_levels])\n",
    "\n",
    "# Annotate AP values\n",
    "for i in range(len(pos_noise_levels)):\n",
    "    for j in range(len(neg_noise_levels)):\n",
    "        plt.text(j, i, f'{ap_matrix[i, j]:.2f}', ha='center', va='center', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172ea2e9-009d-4e22-b7df-8916d75dcdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC AUC\n",
    "plt.figure(figsize=(14, 6), dpi=300)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(roc_auc_matrix, interpolation='nearest', cmap='viridis')\n",
    "plt.title('Increasing Noise in Test Set: \\nROC AUC', fontsize=20)\n",
    "plt.xlabel('Increasing False Positives', fontsize=18)\n",
    "plt.ylabel('Increasing False Negatives', fontsize=18)\n",
    "plt.colorbar(label='ROC AUC')\n",
    "plt.xticks(ticks=np.arange(len(neg_noise_levels)), labels=[f'{nl*100:.0f}%' for nl in neg_noise_levels], fontsize=12)\n",
    "plt.yticks(ticks=np.arange(len(pos_noise_levels)), labels=[f'{pl*100:.0f}%' for pl in pos_noise_levels], fontsize=12)\n",
    "\n",
    "# Annotate ROC AUC values with contrasting color based on background intensity\n",
    "for i in range(len(pos_noise_levels)):\n",
    "    for j in range(len(neg_noise_levels)):\n",
    "        value = roc_auc_matrix[i, j]\n",
    "        color = 'midnightblue' if value > 0.7 else 'white'  # Choose color based on a threshold\n",
    "        plt.text(j, i, f'{value:.2f}', ha='center', va='center', color=color, fontsize=10)\n",
    "\n",
    "# Plot Average Precision\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(ap_matrix, interpolation='nearest', cmap='viridis')\n",
    "plt.title('Increasing Noise in Test Set: \\nAverage Precision', fontsize=20)\n",
    "plt.xlabel('Increasing False Positives', fontsize=18)\n",
    "plt.ylabel('Increasing False Negatives', fontsize=18)\n",
    "plt.colorbar(label='Average Precision')\n",
    "plt.xticks(ticks=np.arange(len(neg_noise_levels)), labels=[f'{nl*100:.0f}%' for nl in neg_noise_levels], fontsize=12)\n",
    "plt.yticks(ticks=np.arange(len(pos_noise_levels)), labels=[f'{pl*100:.0f}%' for pl in pos_noise_levels],fontsize=12)\n",
    "\n",
    "# Annotate AP values with contrasting color based on background intensity\n",
    "for i in range(len(pos_noise_levels)):\n",
    "    for j in range(len(neg_noise_levels)):\n",
    "        value = ap_matrix[i, j]\n",
    "        color = 'midnightblue' if value > 0.7 else 'white'  # Choose color based on threshold\n",
    "        plt.text(j, i, f'{value:.2f}', ha='center', va='center', color=color, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a0328-96f7-4121-b2b1-c56b326d8531",
   "metadata": {},
   "source": [
    "## Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853213a6-f85f-4175-a326-934cf7944974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12), dpi=300)\n",
    "\n",
    "# Top-left: Train ROC AUC (cividis)\n",
    "ax = axes[0,0]\n",
    "im = ax.imshow(roc_auc_matrix_train, interpolation='nearest', cmap='cividis')\n",
    "ax.set_title('Train Set – ROC AUC', fontsize=20)\n",
    "ax.set_xlabel('False Positive Noise', fontsize=18)\n",
    "ax.set_ylabel('False Negative Noise', fontsize=18)\n",
    "ax.set_xticks(np.arange(len(neg_noise_levels_train)))\n",
    "ax.set_xticklabels([f'{nl*100:.0f}%' for nl in neg_noise_levels_train], fontsize=12)\n",
    "ax.set_yticks(np.arange(len(pos_noise_levels_train)))\n",
    "ax.set_yticklabels([f'{pl*100:.0f}%' for pl in pos_noise_levels_train], fontsize=12)\n",
    "cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('ROC AUC', fontsize=14)\n",
    "\n",
    "for i in range(len(pos_noise_levels_train)):\n",
    "    for j in range(len(neg_noise_levels_train)):\n",
    "        val = roc_auc_matrix_train[i, j]\n",
    "        color = 'midnightblue' if val > 0.7 else 'white'\n",
    "        ax.text(j, i, f'{val:.2f}', ha='center', va='center', color=color, fontsize=11)\n",
    "\n",
    "# Top-right: Train AP (cividis)\n",
    "ax = axes[0,1]\n",
    "im = ax.imshow(ap_matrix_train, interpolation='nearest', cmap='cividis')\n",
    "ax.set_title('Train Set – Average Precision', fontsize=20)\n",
    "ax.set_xlabel('False Positive Noise', fontsize=18)\n",
    "ax.set_ylabel('False Negative Noise', fontsize=18)\n",
    "ax.set_xticks(np.arange(len(neg_noise_levels_train)))\n",
    "ax.set_xticklabels([f'{nl*100:.0f}%' for nl in neg_noise_levels_train], fontsize=12)\n",
    "ax.set_yticks(np.arange(len(pos_noise_levels_train)))\n",
    "ax.set_yticklabels([f'{pl*100:.0f}%' for pl in pos_noise_levels_train], fontsize=12)\n",
    "cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Average Precision', fontsize=14)\n",
    "\n",
    "for i in range(len(pos_noise_levels_train)):\n",
    "    for j in range(len(neg_noise_levels_train)):\n",
    "        val = ap_matrix_train[i, j]\n",
    "        color = 'midnightblue' if val > 0.4 else 'white'\n",
    "        ax.text(j, i, f'{val:.2f}', ha='center', va='center', color=color, fontsize=11)\n",
    "\n",
    "# Bottom-left: Test ROC AUC (viridis)\n",
    "ax = axes[1,0]\n",
    "im = ax.imshow(roc_auc_matrix, interpolation='nearest', cmap='viridis')\n",
    "ax.set_title('Test Set – ROC AUC', fontsize=20)\n",
    "ax.set_xlabel('False Positive Noise', fontsize=18)\n",
    "ax.set_ylabel('False Negative Noise', fontsize=18)\n",
    "ax.set_xticks(np.arange(len(neg_noise_levels)))\n",
    "ax.set_xticklabels([f'{nl*100:.0f}%' for nl in neg_noise_levels], fontsize=12)\n",
    "ax.set_yticks(np.arange(len(pos_noise_levels)))\n",
    "ax.set_yticklabels([f'{pl*100:.0f}%' for pl in pos_noise_levels], fontsize=12)\n",
    "cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('ROC AUC', fontsize=14)\n",
    "\n",
    "for i in range(len(pos_noise_levels)):\n",
    "    for j in range(len(neg_noise_levels)):\n",
    "        val = roc_auc_matrix[i, j]\n",
    "        color = 'midnightblue' if val > 0.7 else 'white'\n",
    "        ax.text(j, i, f'{val:.2f}', ha='center', va='center', color=color, fontsize=11)\n",
    "\n",
    "# Bottom-right: Test AP (viridis)\n",
    "ax = axes[1,1]\n",
    "im = ax.imshow(ap_matrix, interpolation='nearest', cmap='viridis')\n",
    "ax.set_title('Test Set – Average Precision', fontsize=20)\n",
    "ax.set_xlabel('False Positive Noise', fontsize=18)\n",
    "ax.set_ylabel('False Negative Noise', fontsize=18)\n",
    "ax.set_xticks(np.arange(len(neg_noise_levels)))\n",
    "ax.set_xticklabels([f'{nl*100:.0f}%' for nl in neg_noise_levels], fontsize=12)\n",
    "ax.set_yticks(np.arange(len(pos_noise_levels)))\n",
    "ax.set_yticklabels([f'{pl*100:.0f}%' for pl in pos_noise_levels], fontsize=12)\n",
    "cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Average Precision', fontsize=14)\n",
    "\n",
    "for i in range(len(pos_noise_levels)):\n",
    "    for j in range(len(neg_noise_levels)):\n",
    "        val = ap_matrix[i, j]\n",
    "        color = 'midnightblue' if val > 0.7 else 'white'\n",
    "        ax.text(j, i, f'{val:.2f}', ha='center', va='center', color=color, fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"label_noise.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144c30a-47e3-452c-9942-481978117950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Load your real matrices here:\n",
    "# roc_auc_matrix_train, ap_matrix_train, roc_auc_matrix, ap_matrix\n",
    "# pos_noise_levels_train, neg_noise_levels_train\n",
    "# pos_noise_levels, neg_noise_levels\n",
    "\n",
    "# Configure font\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size']   = 16\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12), dpi=300)\n",
    "\n",
    "def plot_heatmap(ax, matrix, title, cmap, pos_levels, neg_levels, val_threshold, summary_text):\n",
    "    im = ax.imshow(matrix, interpolation='nearest', cmap=cmap)\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ax.set_xlabel('False Positive Noise', fontsize=18)\n",
    "    ax.set_ylabel('False Negative Noise', fontsize=18)\n",
    "    ax.set_xticks(np.arange(len(neg_levels)))\n",
    "    ax.set_xticklabels([f'{nl*100:.0f}%' for nl in neg_levels], fontsize=12)\n",
    "    ax.set_yticks(np.arange(len(pos_levels)))\n",
    "    ax.set_yticklabels([f'{pl*100:.0f}%' for pl in pos_levels], fontsize=12)\n",
    "    cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(title.split('–')[-1].strip(), fontsize=14)\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            val = matrix[i, j]\n",
    "            color = 'midnightblue' if val > val_threshold else 'white'\n",
    "            ax.text(j, i, f'{val:.2f}', ha='center', va='center', color=color, fontsize=11)\n",
    "\n",
    "    # Centered callout at top\n",
    "    ax.text(0.5, 0.98, summary_text, transform=ax.transAxes,\n",
    "            fontsize=12, va='top', ha='center',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.4\", fc=\"white\", ec=\"black\", alpha=0.8))\n",
    "\n",
    "# Plot each subplot with centered callout\n",
    "plot_heatmap(\n",
    "    axes[0,0],\n",
    "    roc_auc_matrix_train,\n",
    "    'Train Set – ROC AUC',\n",
    "    cmap='cividis',\n",
    "    pos_levels=pos_noise_levels_train,\n",
    "    neg_levels=neg_noise_levels_train,\n",
    "    val_threshold=0.7,\n",
    "    summary_text=\"Lighter = better discrimination\\nAUC > 0.5 better-than-chance performance\"\n",
    ")\n",
    "\n",
    "plot_heatmap(\n",
    "    axes[0,1],\n",
    "    ap_matrix_train,\n",
    "    'Train Set – Average Precision',\n",
    "    cmap='cividis',\n",
    "    pos_levels=pos_noise_levels_train,\n",
    "    neg_levels=neg_noise_levels_train,\n",
    "    val_threshold=0.4,\n",
    "    summary_text=\"Lighter = better precision-recall\"\n",
    ")\n",
    "\n",
    "plot_heatmap(\n",
    "    axes[1,0],\n",
    "    roc_auc_matrix,\n",
    "    'Test Set – ROC AUC',\n",
    "    cmap='viridis',\n",
    "    pos_levels=pos_noise_levels,\n",
    "    neg_levels=neg_noise_levels,\n",
    "    val_threshold=0.7,\n",
    "    summary_text=\"Lighter = better discrimination\\nAUC > 0.5 better-than-chance performance\"\n",
    ")\n",
    "\n",
    "plot_heatmap(\n",
    "    axes[1,1],\n",
    "    ap_matrix,\n",
    "    'Test Set – Average Precision',\n",
    "    cmap='viridis',\n",
    "    pos_levels=pos_noise_levels,\n",
    "    neg_levels=neg_noise_levels,\n",
    "    val_threshold=0.6,\n",
    "    summary_text=\"Lighter = better precision-recall\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"label_noise_callout.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d428143-497b-4ceb-be17-f0885e434b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
